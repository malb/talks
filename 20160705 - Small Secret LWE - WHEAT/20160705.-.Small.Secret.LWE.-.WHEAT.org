#+TITLE: Some Remarks on Small Secret LWE
#+SUBTITLE:  Another Look at HELib’s Choices of Parameters
#+AUTHOR: Martin R. Albrecht
#+EMAIL: @martinralbrecht
#+DATE: 07/05/2016
#+STARTUP: beamer indent

#+OPTIONS: H:2 toc:t num:t todo:t
#+LANGUAGE: en
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport

#+LaTeX_CLASS: mbeamer
#+LaTeX_HEADER: \newcommand{\ZZ}[1][blank]{\ensuremath{\ifthenelse{\equal{#1}{blank}}{\mathbb{Z}}{\mathbb{Z}\left[#1\right]}\xspace}}
#+LaTeX_HEADER: \newcommand{\QQ}[1][blank]{\ensuremath{\ifthenelse{\equal{#1}{blank}}{\mathbb{Q}}{\mathbb{Q}\left[#1\right]}\xspace}}
#+LaTeX_HEADER: \newcommand{\ZZq}[1][blank]{\ensuremath{\ifthenelse{\equal{#1}{blank}}{\mathbb{Z}_q}{\mathbb{Z}_q\left[#1\right]}\xspace}}
#+LATEX_HEADER: \usepackage{filecontents}
#+LATEX_HEADER: \renewcommand{\U}[1]{\ensuremath{\mathcal{U}\left( {#1} \right)}\xspace}
#+LATEX_HEADER: \newcommand{\mat}[1]{\ensuremath{\mathbf{#1}}\xspace}
#+LATEX_HEADER: \renewcommand{\vec}[1]{\ensuremath{\mathbf{#1}}\xspace}
#+LATEX_HEADER: \newcommand{\shortvec}[1]{\tilde{\mathbf{#1}}\xspace}
#+LATEX_HEADER: \newcommand{\sample}{\ensuremath{\leftarrow_{\$}}}
#+LATEX_HEADER: \newcommand{\ovec}[1]{\ensuremath{\overline{\vec{#1}}}\xspace}
#+LATEX_HEADER: \newcommand{\Id}[1][n]{\ensuremath{\mat{I}_{#1}}\xspace}
#+LATEX_HEADER: \newcommand{\Ze}[1][n \times n]{\ensuremath{\mat{0}_{#1}}\xspace}
#+LATEX_HEADER: \renewcommand{\B}[2][]{\ensuremath{\mathcal{B}_{#1}^{#2}}\xspace}
#+LATEX_HEADER: \newcommand{\ip}[2]{\ensuremath{\left\langle {#1},{#2}\right\rangle}\xspace}
#+LATEX_HEADER: \definecolor{lightblue}{HTML}{4B8EC8}
#+LATEX_HEADER: \definecolor{butter1}{rgb}{0.988,0.914,0.310}
#+LATEX_HEADER: \definecolor{chocolate1}{rgb}{0.914,0.725,0.431}
#+LATEX_HEADER: \definecolor{chameleon1}{rgb}{0.541,0.886,0.204}
#+LATEX_HEADER: \definecolor{skyblue1}{rgb}{0.447,0.624,0.812}
#+LATEX_HEADER: \definecolor{plum1}{rgb}{0.678,0.498,0.659}
#+LATEX_HEADER: \definecolor{scarletred1}{rgb}{0.937,0.161,0.161}
#+LATEX_HEADER: \setbeamercolor{example text}{fg=mDarkBrown}
#+LATEX_HEADER: \newcommand{\pq}{\ensuremath{\frac{p}{q}}}
#+LATEX_HEADER: \newcommand{\round}[1]{\ensuremath{\left\lfloor{#1}\right\rceil}\xspace}
#+LATEX_HEADER: \newcommand{\Ldis}{L_{\vec{s},\chi}^{(n)}\xspace}
#+LATEX_HEADER: \newcommand{\Q}[1][⋅]{\ensuremath{\mathcal{Q}_{\vec{s}}\left( {#1} \right)}\xspace}

#+COLUMNS: %45ITEM %10BEAMER_ENV(Env) %4BEAMER_ACT(Act) %4BEAMER_COL(Col) %4BEAMER_OPT(Opt)
#+BIBLIOGRAPHY: local.bib,abbrev3.bib,crypto_crossref.bib

* Code                                                                :noexport:

Set up of common variables.

#+BEGIN_SRC sage
load("estimator/estimator.py")
load("small_sparse_secret.py")
n, alpha, q = fhe_params(n=2048, L=2)
h = 64

chocolate1 = (233/255.0,185/255.0,110/255.0)
skyblue1 = (114/255.0, 159/255.0, 207/255.0)
#+END_SRC

#+RESULTS:

* Introduction
** Learning with Errors

The Learning with Errors (LWE) problem was deﬁned by Oded Regev.footfullcite:STOC:Regev05

Given $(\vec{A},\vec{c})$ with $\vec{c} \in \ZZq^{m}$, $\vec{A} \in \ZZq^{m × n}$, $\vec{s} \in \ZZq^{n}$ and small $\vec{e} \in \ZZ^{m}$ is

#+BEGIN_EXPORT LaTeX
\[
\left(\begin{array}{c}
\\
\\
\\ 
\vec{c} \\
\\
\\
\\
\end{array} \right) = \left(
\begin{array}{ccc}
\leftarrow & n & \rightarrow \\
\\
\\ 
& \vec{A} & \\
\\
\\
\\
\end{array} \right) \cdot \left( \begin{array}{c}
\\
\vec{s} \\
\\
\end{array} \right) + \left(
\begin{array}{c}
\\
\\
\\ 
\vec{e} \\
\\
\\
\\
\end{array} 
\right)
\]
#+END_EXPORT

or $\vec{c} \sample \U{\ZZq^{m}}$.

** Parameters

***                                                           :B_column:BMCOL:
:PROPERTIES:
:BEAMER_env: column
:BEAMER_col: 0.5
:END:

#+BEGIN_EXPORT LaTeX
\begin{tikzpicture}[scale=0.7]
  \begin{axis}[
    domain=-10:10,
    grid=major,smooth,
    xlabel=$x$,
    ylabel=$\approx \textnormal{Pr}(x)$,
    ]
    \addplot[color=chocolate1,very thick,samples=50,smooth]{exp(-(x^2)/18)};
    \addplot[only marks,color=lightblue] coordinates {
      (-9, 0.011)
      (-8, 0.028)
      (-7, 0.065)
      (-6, 0.135)
      (-5, 0.249)
      (-4, 0.411)
      (-3, 0.606)
      (-2, 0.800)
      (-1, 0.945)
      (0, 1.000)
      (1, 0.945)
      (2, 0.800)
      (3, 0.606)
      (4, 0.411)
      (5, 0.249)
      (6, 0.135)
      (7, 0.065)
      (8, 0.028)
      (9, 0.011)
    };
  \end{axis}
\end{tikzpicture}
#+END_EXPORT


***                                                                    :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:

- Parameters are: 
  - dimension $n$, 
  - modulus $q$ (e.g. $q \approx n^2$), 
  - noise size $\alpha$ (e.g. $\alpha q \approx \sqrt{n}$),
  - number of samples $m$.

- Elements of $\vec{A}, \vec{s}, \vec{e}, \vec{c}$ are in $\ZZ_q$.
- $\vec{e}$ is sampled from $χ_{α}$, a discrete Gaussian with width \[\sigma=\frac{\alpha q}{\sqrt{2 \pi}}.\]

** FHE-schemes based on LWE

#+BEAMER: \footnotesize

- BGV ::  fullcite:ITCS:BraGenVai12
- FV :: fullcite:EPRINT:FanVer12
- LTV :: fullcite:STOC:LopTroVai12 [fn:1]
- YASHE :: fullcite:IMA:BLLN13
 
** FHE-schemes based on LWE (cont.)

#+BEAMER: \footnotesize

- GSW :: fullcite:C:GenSahWat13

- AGCD :: fullcite:EC:CheSte15

** Small Secrets

- FHE schemes based on LWE typically choose very small secrets.
- For example, $\vec{s}_{i} ← \{-1,0,1\}$ or $\vec{s_{i}} ← \{0,1\}$.
- =HElib= footfullcite:C:HalSho14 typically chooses $\vec{s}$ such that $w=64$ entries are $±1$ and all remaining entries are $0$, regardless of dimension $n$.
- The same strategy is used in a recent comparison study.footfullcite:RSA:CosSma16

#+BEGIN_CENTER
*How many bits of security does this cost?*
#+END_CENTER

** Binary LWE Secret Distributions

- $\B{+}$ :: each component is independently sampled uniformly from \(\{0,1\}\).
- $\B{-}$ :: each component is independently sampled uniformly from \(\{-1,0,1\}\).
- $\B[h]{±}$ :: like above but with guarantee that \(h\) components are non-zero.

** Hardness: LWE Normal Form

Given samples
\[(\vec{a},c)=(\vec{a},\langle\vec{a},\alert{\vec{s}}\rangle+ e) \in \ZZq^n × \ZZq\]
with $\vec{a} \gets \mathcal{U}(\ZZq^n)$, $e \gets D_{α q,0}$ and $\vec{s} \in \ZZq^n$,
we can construct samples
\[(\vec{a}, c)=(\vec{a},⟨\vec{a},\alert{\vec{e}}⟩+ e) \in \ZZq^n × \ZZq\] with
$\vec{a} ← \mathcal{U}(\ZZq^n)$, $e ← D_{α q, 0}$ and *$\vec{e}$* such that all components *\[e_i ← D_{α q, 0}\]* in polynomial time.footfullcite:C:ACPS09

** Hardness: Reductions

#+BEGIN_QUOTE
“A major part of our reduction [\dots] is therefore dedicated to showing  reduction from LWE (in dimension $n$) with arbitrary secret in $\ZZq^n$ to LWE (in dimension *$n \log_2 q$*) with a secret chosen uniformly over $\{0, 1\}$.” footfullcite:STOC:BLPRS13
#+END_QUOTE

** Hardness: Algorithms

#+BEGIN_QUOTE
“[This work] suggests that this is overkill and that even *$n\log\log n$* may be more than sufficient.”footfullcite:ACISP:BaiGal14
#+END_QUOTE

** Hardness: Constructions

#+BEGIN_QUOTE
“This brings up the question of whether one can get better attacks against LWE instances with a very sparse secret (much smaller than even the noise). [\dots] it seems that the very sparse secret should only add maybe *one bit to the modulus/noise ratio*.” footfullcite:EPRINT:GenHalSma12
#+END_QUOTE

* Base Line
** Dual Attack

*** Short Integer Solutions (SIS)
Given $q \in \ZZ$, a matrix $\vec{A}$, and $t < q$; find $\vec{y}$ with $0 < \| \vec{y} \| \leq t$ and \[\vec{y} ⋅ \vec{A} ≡ \vec{0} \pmod{q}.\]

***                                                          :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:
- Find a short $\vec{y}$ solving SIS on $\vec{A}$.
- Given LWE samples $\vec{A}, \vec{c}$ where $\vec{c} = \vec{A}⋅\vec{s} + \vec{e}$ or $\vec{c}$ uniform.
- Compute $\ip{\vec{y}}{\vec{c}}$.
  - If $\vec{c} = \vec{A}⋅\vec{s} + \vec{e}$, then $\ip{\vec{y}}{\vec{c}} = \ip{\vec{y}⋅\vec{A}}{\vec{s}} + \ip{\vec{y}}{\vec{e}} \equiv \ip{\vec{y}}{\vec{e}} \pmod{q}$.
  - If $\vec{c}$ is uniformly random, so is $\ip{\vec{y}}{\vec{c}}$.
- If $\vec{y}$ is short then $\ip{\vec{y}}{\vec{e}}$ is also short.

** Dual Attack

*** 
:PROPERTIES:
:BEAMER_env: lemma
:END:

Given an LWE instance characterised by $n$, $α$, $q$ and a vector $\vec{v}$ of length $\|\vec{v}\|$ such that $\vec{v} ⋅ \vec{A} \equiv 0 \pmod{q}$, the advantage of distinguishing $\ip{\vec{v}}{\vec{e}}$ from random is close to footfullcite:RSA:LinPei11 \[\exp(-π(\|\vec{v}\| ⋅ α)^2).\]

** Dual Attack

A *reduced lattice* basis contains short vectors. In particular, the first vector is short: $\|\vec{v}\| ≈ δ_0^m\, q^{n/m}$.

1. Construct a basis of the dual lattice from $\vec{A}$.
2. Run lattice reduction algorithm to obtain short vectors $\vec{v}_i$.
3. Check if $\vec{v}_i⋅ \vec{A}$ are small.footfullcite:PQCBook:MicReg09

*** Cost                                                             :B_block:
:PROPERTIES:
:BEAMER_env: block
:BEAMER_act: <2->
:END:

How expensive is it to achieve the target quality?

** HELib

#+BEGIN_SRC C++
long FindM(long k, long L, long c, long p, long d, long s,
           long chosen_m, bool verbose) {
  // get a lower-bound on the parameter N=phi(m):
  …
  // 6. To get k-bit security we need N>log(Q0/sigma)(k+110)/7.2, i.e.
  //    roughly N > (L+1)*pSize*(1+1/c)(k+110) / 7.2

  // Compute a bound on m, and make sure that it is not too large
  double cc = 1.0+(1.0/(double)c);
  double dN = ceil((L+1)*FHE_pSize*cc*(k+110)/7.2);
  …
  return m;
}
#+END_SRC

** Lindner-Peikert Estimates

Lindner and Peikert footfullcite:RSA:LinPei11 give an estimate for the runtime (in seconds) of BKZ as \[\log{t_{BKZ}(δ_0)} = \frac{1.8}{\log{δ_0}}-110\] based on experiments with BKZ in the NTL library.

** Lindner-Peikert Estimates

- The LP model does not fit the implementation of BKZ in NTL.

- NTL does not implement preprocessing of local blocks with BKZ recursively.[fn:2]

- Hence, its enumeration requires $2^{Ω(k^2)}$ time in block size $k$.

** Lindner-Peikert Estimates

The LP model assumes a linear relation between $1/k$ and $\log(δ_0)$, but from the “lattice rule-of-thumb” ($δ_0 ≈ k^{1/(2k)}$) we get footfullcite:JMC:AlbPlaSco15

***                                                                  :B_lemma:
:PROPERTIES:
:BEAMER_env: lemma
:END:

The log of the time complexity achieve a root-Hermite factor $δ_0$ with BKZ is
\[Ω \left( \frac{\log(1/\log δ_0)}{\log δ_0} \right)\]
if calling the SVP oracle costs \(2^{Ω(k)}\).

** LP = A Subexponential Attack on Regev’s LWE

*** 
:PROPERTIES:
:BEAMER_env: lemma
:END:

Given an LWE instance parametrised by $n$, $q=n^c$, $αq = \sqrt{n}$. A lattice reduction algorithm achieving log root-Hermite factor
\[\log δ_0 = {\frac{\left(\left(c-\frac{1}{2} \right) \log{n} + \log{\sqrt{\ln(1/ε)/π}} \right)^2}{4cn \log{n}} }\] can be used to distinguish the LWE distribution with advantage $ε$.footfullcite:JMC:AlbPlaSco15

***                                                          :B_ignoreheading:
:PROPERTIES:
:BEAMER_env: ignoreheading
:END:

Picking $\log{\sqrt{\ln(1/ε)/π}} ≈ 1$ and $c=2$ we gets 

\[\log δ_0 = \frac{9\, \log n }{32\,n} \textnormal{ and } \log \left(t_{BKZ}(δ_0)\right) = \frac{32\, n}{5\, \log n }-110.\]

** BKZ 2.0 Based Guestimates

We’ll assume footfullcite:PhD:Chen13,C:Laarhoven15,C:HanPujSte11

- $δ_0 ≈ {\left( \frac{k}{2 \pi e} {(π k)}^{\frac{1}{k}}  \right)}^{\frac{1}{2(k-1)}}$ 
- sieving is used as the SVP oracle in dimension $k$
- sieving in blocksize $k$ costs $t_k = 2^{0.3366\,k + 12.31}$ clock cycles 
- BKZ-$k$ costs $\frac{n^3}{k^2} \log(n) \cdot t_k$ cycles

***  Samples
:PROPERTIES:
:BEAMER_env: block
:END:

We will also assume access to as many samples as needed.

** Comparison

#+BEGIN_SRC sage :file lp_vs_sieve.png :exports results
kwds = {"figsize":[12, 6], "thickness":2, "dpi":300r, "transparent":True, "axes_labels": ['$\delta_0$','$\log_2(cost)$']}

delta_list = [1.004+i/10000.0 for i in range(1,55)]
l1 = line([(delta,bkz_runtime_delta_LP(delta, 1024)) for delta in delta_list],
          rgbcolor=chocolate1, legend_label="lp", **kwds)
l2 = line([(delta,bkz_runtime_k_sieve(k_chen(delta), 1024)) for delta in delta_list], 
          rgbcolor=skyblue1, legend_label="sieve", **kwds)
l1+l2
#+END_SRC

#+ATTR_LATEX: :width 1.0\textwidth
#+RESULTS:
[[file:lp_vs_sieve.png]]


** Rolling Example

We use the following LWE parameters as a rolling example throughout this talk.

  - dimension $n=2048$,
  - modulus $q ≈ 2^{63.4}$,
  - noise parameter $\alpha ≈ 2^{-60.4}$, i.e. standard deviation $σ ≈ 3.2$,
  - $h=64$ components of the secret are $\pm 1$, all other components are zero, $σ_s ≈ 0.44$: $\B[64]{-}$

This is inspired by parameters choices in =HElib=.

** Lattice Attacks

- Dual Attack :: solve Short Integer Solutions problem (SIS) in the left kernel of $\vec{A}$, i.e.
  \[
  \textnormal{ find a short } \vec{w} \textnormal{ such that } \vec{w} ⋅ \vec{A} = 0
  \]
  and check if $\ip{\vec{w}}{\vec{c}} = \vec{w}⋅ \left(\vec{A} ⋅ \vec{s} + \vec{e}\right) = \ip{\vec{w}}{\vec{e}}$ is short.

- Primal Attack :: solve Bounded Distance Decoding problem (BDD), i.e.
  \[
  \textnormal{ find } \vec{s'} \textnormal{ s.t. } \|\vec{w} - \vec{c}\| \textnormal{ with } \vec{w} = \vec{A} ⋅ \vec{s'} \textnormal{ is minimised}
  \]
  using 
  - Kannan's embedding or

  - Babai's nearest planes (Decoding).

** Base Line

#+BEGIN_CENTER
#+BEGIN_EXPORT LaTeX
\begin{tikzpicture}[scale=0.95]
  \begin{axis}[
    legend pos=outer north east,
    axis lines=middle,
    xmin=0.5, xmax=3.5,
    ymin=80, ymax=190,
    axis y line*=left,
    axis x line*=bottom,
    xticklabels={Dual, Decode, Kannan},
    xtick={1,...,3},
    ytick={100, 137.4, 174.6, 200},
    x tick label style={rotate=45,anchor=east},
    legend columns=1,
    legend cell align=left,]
    \addplot [only marks,color=skyblue1] table [x=idx,y=base/sieve,col sep=comma] {times.csv};
    \addplot [only marks,color=chocolate1,] table [x=idx,y=base/lp,col sep=comma] {times.csv};
    \legend{sieving,lp}\;
    \addplot[color=lightgray,table/row sep=\\] table {
      0.5 137.4\\
      3.5 137.4\\
    };
    \addplot[color=lightgray,table/row sep=\\] table {
      0.5 174.6\\
      3.5 174.6\\
    };

  \end{axis}
\end{tikzpicture}
#+END_EXPORT
#+END_CENTER

*** Code                                                            :noexport:

#+BEGIN_SRC sage
SIS = sis(n, alpha, q, optimisation_target="sieve")
DEC = decode(n, alpha, q, optimisation_target="sieve")
BDD = kannan(n, alpha, q, optimisation_target="sieve")
log(SIS["sieve"],2).n(), log(DEC["bop"],2).n(), log(BDD["sieve"],2).n()
#+END_SRC

#+RESULTS:
: (142.627800153992, 137.351292489130, 138.789506345919)

#+BEGIN_SRC sage
SIS = sis(n, alpha, q, optimisation_target="lp")
DEC = decode(n, alpha, q, optimisation_target="lp")
BDD = kannan(n, alpha, q, optimisation_target="lp")
log(SIS["lp"],2).n(), log(DEC["bop"],2).n(), log(BDD["lp"],2).n()
#+END_SRC

#+RESULTS:
: (185.920086500506, 174.673928471568, 178.647243417037)

* Swapping Error and Secret
** Swapping Error and Secret

#+BEGIN_QUOTE
“applying the reduction technique of Applebaum et al.footfullcite:C:ACPS09 to switch the key with part of the error vector, thus getting a smaller LWE error.”footfullcite:EPRINT:GenHalSma12
#+END_QUOTE

** Swapping Error and Secret

- Let $\vec{A}_0$ denotes the first $n$ rows of $\vec{A}$, $\vec{A}_1$ the next $n$ rows, etc.

- $\vec{e}_0, \vec{e}_1, \dots$ are the corresponding parts of the error vector and

- $\vec{c}_0 , \vec{c}_1, \dots$ the corresponding parts of $\vec{c}$.

- For $i=0$ we have \(\vec{c}_0 = \vec{A}_0 \cdot \vec{s}  + \vec{e}_0\) or \[\vec{A}_0^{-1} \cdot \vec{c}_0 = \vec{s} + \vec{A}_0^{-1} \vec{e}_0.\]

- For $i > 0$ we have \(\vec{c}_i = \vec{A}_i \cdot \vec{s} + \vec{e}_i\), which together with the above gives \[\vec{A}_i ⋅ \vec{A}_0^{-1} ⋅ \vec{c}_0 - \vec{c}_i = \vec{A}_i ⋅ (\vec{s} + \vec{A}_0^{-1} \vec{e}_0) - \vec{c}_i =  \vec{A}_i ⋅ \vec{A}_0^{-1} \vec{e}_0 - \vec{e}_i.\]

** Bai-Gal Algorithm

- Consider the lattice
  \[Λ=\{\vec{v} \in \ZZ^{n+m} | (\vec{A} | \vec{I}_m ) ⋅ \vec{v} ≡ 0 \pmod{q} \}\]
- It has an unusually short vector $(\vec{s} || \vec{e})$.
- When $\|\vec{s}\| \ll \|\vec{e}\|$, the vector $(\vec{s} || \vec{e})$ is uneven in length.
- Rescale the first part to have the same norm as the second.footfullcite:ACISP:BaiGal14 @@beamer:\pause@@
  - When $\vec{s} \sample \B{-}$, the volume of the lattice is scaled by $\sigma^n$.
  - When $\vec{s} \sample \B{+}$ the volume of the lattice is scaled by ${(2\sigma)}^n$ because we can scale by $2\sigma$ and then rebalance.
  - When $\vec{s} \sample \B[hw]{±}$ the volume is scaled depending on the $hw$.

** Swapping Error and Secret: Sieving

#+BEGIN_CENTER
#+BEGIN_EXPORT LaTeX
\begin{tikzpicture}[scale=0.95]
  \begin{axis}[
    legend pos=outer north east,
    axis lines=middle,
    xmin=0.5, xmax=3.5,
    ymin=80, ymax=160,
    axis y line*=left,
    axis x line*=bottom,
    xticklabels={Dual, Decode, Embed},
    xtick={1,...,3},
    ytick={100, 134.8, 150},
    x tick label style={rotate=45,anchor=east},
    legend columns=1,
    legend cell align=left,]
    \addplot [only marks,color=skyblue1!40!white] table [x=idx,y=base/sieve,col sep=comma] {times.csv};
    \addplot [only marks,color=skyblue1]  table [x=idx,y=sec/sieve,col sep=comma] {times.csv};
    \legend{no swap, swap}\;
    \addplot[color=lightgray,table/row sep=\\] table {
      0.5 134.8\\
      3.5 134.8\\
    };

  \end{axis}
\end{tikzpicture}
#+END_EXPORT

For our rolling example this reduces $α$ from $2^{-60.4}$ to $≈2^{-60.8}$
#+END_CENTER

*** Code                                                            :noexport:

#+BEGIN_SRC sage
load("estimator/estimator.py")
load("small_sparse_secret.py")
n, alpha, q = fhe_params(n=2048, L=2)
n, alpha, q = applebaum_transform(n, alpha, q, m=2*n, secret_bounds=(-1,1), h=64)
SIS = sis(n, alpha, q, optimisation_target="sieve")
DEC = decode(n, alpha, q, optimisation_target="sieve")
BDD = kannan(n, alpha, q, optimisation_target="sieve")
log(SIS["sieve"],2).n(), log(DEC["bop"],2).n(), log(BDD["sieve"],2).n()
#+END_SRC

#+RESULTS:
: (140.306126345015, 134.976498206719, 136.798452527420)

#+BEGIN_SRC sage
load("estimator/estimator.py")
n, alpha, q = fhe_params(n=2048, L=2)
n, alpha, q = applebaum_transform(n, alpha, q, m=2*n, secret_bounds=(-1,1), h=64)
SIS = sis(n, alpha, q, optimisation_target="lp")
DEC = decode(n, alpha, q, optimisation_target="lp")
BDD = kannan(n, alpha, q, optimisation_target="lp")
log(SIS["lp"],2).n(), log(DEC["lp"],2).n(), log(BDD["lp"],2).n()
#+END_SRC

#+RESULTS:
: (182.476446140684, 171.314275391242, 175.336929727914)

* Modulus Switching
** Modulus Switching

***                                                                  :B_lemma:
:PROPERTIES:
:BEAMER_env: lemma
:END:

Let $(\vec{a},c) =(\vec{a}, \ip{\vec{a}}{\vec{s}} + e) \in \ZZq^n × \ZZq$ be an LWE sample and \[p ≈ \sqrt{\frac{2π\, n}{12}} ⋅ \frac{σ_s}{α},\] where $σ_s$ is the standard deviation of components of $\vec{s}$. If $p<q$ then \[\bigg(\round{\frac{p}{q} ⋅ \vec{a}}, \round{\frac{p}{q} ⋅  c}\bigg) \textnormal{ in } \ZZ_{p}^n × \ZZ_{p}\] follows a distribution close to an LWE distribution with $n, \sqrt{2}\,α, p$.footfullcite:FOCS:BraVai11

** Modulus Switching in Cryptanalysis

When the secret is much smaller than the noise, applying modulus switching produces an easier LWE problem.

** Modulus Switching: Sieving

#+BEGIN_CENTER
#+BEGIN_EXPORT LaTeX
\begin{tikzpicture}[scale=0.95]
  \begin{axis}[
    legend pos=outer north east,
    axis lines=middle,
    xmin=0.5, xmax=3.5,
    ymin=120, ymax=150,
    axis y line*=left,
    axis x line*=bottom,
    xticklabels={Dual, Decode, Embed},
    xtick={1,...,3},
    ytick={120, 137.4, 150},
    x tick label style={rotate=45,anchor=east},
    legend columns=1,
    legend cell align=left,]
    \addplot [only marks,color=skyblue1!40!white] table [x=idx,y=base/sieve,col sep=comma] {times.csv};
    \addplot [only marks,color=skyblue1] table [x=idx,y=mod/sieve,col sep=comma] {times.csv};
    \legend{base line, mod switch}\;
    \addplot[color=lightgray,table/row sep=\\] table {
      0.5 137.4\\
      3.5 137.4\\
    };
  \end{axis}
\end{tikzpicture}
#+END_EXPORT
#+END_CENTER

** Modulus Switching in Combinatorial Dual Attack 

- BKW can seen as a combinatorial version of the Dual Attack.
- It was originally proposed for Learning Parity with Noise (LPN) which can be viewed as a special case of LWE over $\ZZ_{2}$.
- For BKW, variants of modulus switching lead to big performance gains.

** BKW Algorithm

Assume $(\vec{a}_{21},\vec{a}_{22}) = (0, 1)$, then:

#+BEGIN_LATEX
\footnotesize
\begin{align*}
   & \left(
      \begin{array}{rr|rrr|r}
        \phantom{xn}\vec{a}_{11}         & \phantom{xn}\vec{a}_{12}         & \vec{a}_{13} & \cdots & \vec{a}_{1n} & c_1\\
        \alert{\vec{a}_{21}} & \alert{\vec{a}_{22}} & \vec{a}_{23} & \cdots & \vec{a}_{2n} & c_2\\
        \vdots               & \vdots               & \ddots       & \vdots & \vdots\\
        \vec{a}_{m1}         & \vec{a}_{m2}         & \vec{a}_{m3} & \cdots & \vec{a}_{mn} & c_{m}
      \end{array}
                                                                               \right)\\
  -& \left[
      \begin{array}{rr|rrr|r}
        0         & 0         & \vec{t}_{13}   & \cdots & \vec{t}_{1n}   & c_{t,1}\\
        \alert{0} & \alert{1} & \vec{t}_{23}   & \cdots & \vec{t}_{2n}   & c_{t,2}\\
        \vdots    & \vdots    & \ddots         & \vdots & \vdots\\
        q-1       & q-1       & \vec{t}_{q^23} & \cdots & \vec{t}_{q^2n} & c_{t,q^2}
        \end{array}\right]\\
   \Rightarrow &
     \left(\begin{array}{rr|rrr|r}
             \phantom{xn}\vec{a}_{11} & \phantom{xn}\vec{a}_{12} & \vec{a}_{13}      & \cdots & \vec{a}_{1n}      & \tilde{c}_1\\
             \alert{0}    & \alert{0}    & \shortvec{a}_{23} & \cdots & \shortvec{a}_{2n} & \tilde{c}_2\\
             \vdots       & \vdots       & \ddots            & \vdots & \vdots\\
             \vec{a}_{m1} & \vec{a}_{m2} & \vec{a}_{m3}      & \cdots & \vec{a}_{mn}      & c_{m}
           \end{array}\right)
  \end{align*}
#+END_LATEX

** Lazy Modulus Switching

- Create elimination tables which only eliminate the most significant bits
- As a consequence columns are not reduced to zero but to small entries.
- This can be seen as a lazy variant of modulus switching.footfullcite:PKC:AFFP14

- @@beamer:<2->@@ When eliminating higher order bits in columns with bigger indices, the noise of already reduced columns grows back.

** Uneven Noise Contribution

#+BEGIN_EXPORT LaTeX
\begin{center}
\begin{tikzpicture}[scale=0.6,every node/.style={scale=0.6}]
\node (a1) at (0,0) {$(-1, -9 | \phantom{-}7, -9 | -1, \phantom{-}6) \quad -  \quad (-2, -9 | -5, \phantom{-}9 | -5,-4)$};
\node[below of=a1] (e1) {$=$};
\node[below of=e1] (a2)  {$(\alert{\phantom{-}1, \phantom{-}0}| -7,\phantom{-}1|\phantom{-}4,-9)$};

\node (a3) at (10,0) {$(\phantom{-}3, -1|\phantom{-} 0, \phantom{-}0| \phantom{-}2, \phantom{-}6) \quad -  \quad (\phantom{-}4,6|-2,\phantom{-}7| -4, -9)$};
\node[below of=a3] (e3) {$=$};
\node[below of=e3] (a4)  {$(\alert{-1,\phantom{-}1}|-6,\phantom{-}2|\phantom{-}6,-\phantom{-}4)$};

\node at ($(a1)!0.5!(a3)$) {\phantom{(},\phantom{)}};

\node (m1) at ($(a4)!0.5!(a2)$) {$-$};
\node[below of=m1] (e5) {$=$};
\node[below of=e5] (a5) {$(\alert{\phantom{-}2,-1|-1,-1}|\phantom{-}2,\phantom{-}5) $};
\end{tikzpicture}
\end{center}
#+END_EXPORT

** Balancing Noise

- *Pick decreasing moduli* (increasing noise levels) for consecutive blocks to address this problem.
- Complexity now dominated by the size of the first table for eliminating first components.
- To compensate for this, *choose increasing blocksizes* $b_i$ for each block. footfullcite:C:KirFou15

** Coded-BKW

This approach can be generalised

- Consider modulus switching as a special form of quantisation (also done in cite:C:KirFou15)
- Choose appropriate *lattice code* to find good quantisation
- Consider blocks of size $b_i$ as messages which are thrown into buckets based on the codeword they correspond to.footfullcite:C:GuoJohSta15
  
** Coded-BKW

#+BEGIN_CENTER
#+BEGIN_LATEX
\begin{tikzpicture}[scale=0.95]
  \begin{axis}[
    legend pos=outer north east,
    axis lines=middle,
    xmin=0, xmax=4,
    ymin=50, ymax=360,
    axis y line*=left,
    axis x line*=bottom,
    xticklabels={Dual, Decode, Embed, BKW},
    xtick={1,...,4},
    ytick={100, 137.4, 174.7, 359.9},
    x tick label style={rotate=45,anchor=east},
    legend columns=1,
    legend cell align=left,]

    \addplot [only marks,color=skyblue1] table [x=idx,y=base/sieve,col sep=comma] {times.csv};
    \addplot [only marks,color=chocolate1,] table [x=idx,y=base/lp,col sep=comma] {times.csv};

    \addplot [only marks,color=scarletred1,table/row sep=\\] table {
      4 359.9\\
    };
    \legend{lp,sieving,bkw}\;
    \addplot[color=lightgray,table/row sep=\\] table {
      0 137.4\\
      4 137.4\\
    };
    \addplot[color=lightgray,table/row sep=\\] table {
      0 174.7\\
      4 174.7\\
    };

  \end{axis}
\end{tikzpicture}
#+END_EXPORT
#+END_CENTER

#+BEGIN_CENTER
Plain BKW costs *$2^{1310.4}$* bit operations.
#+END_CENTER

** Modulus Switching for Dual Attack

- Lazy modulus switching proceeds from the observation that we do not need to find $\vec{v} ⋅ \vec{A} ≡ 0 \bmod q$, but any short enough $\vec{v} ⋅ \vec{A}$ suffices.
- Consider the dual attack lattice for the LWE normal form \[Λ(\vec{A}) = \{(\vec{x},\vec{y}) \in \ZZ^m × \ZZ^n : \vec{x}⋅ \vec{A} ≡ \vec{y} \bmod q\} \]
- Given a short vector $\vec{v} = (\vec{v}',\vec{w}') \in Λ(\vec{A})$ compute \[\vec{v'}⋅\vec{c} = \vec{v'}⋅(\vec{A}⋅\vec{s} + \vec{e}) = \ip{\vec{w}'}{\vec{s}} + \ip{\vec{v}'}{\vec{e}} \]

** Modulus Switching for Dual Attack

- Aim is to balance \(\|\ip{\vec{w}'}{\vec{s}}\| ≈ \|\ip{\vec{v}'}{\vec{e}}\|\) when $\|\vec{s}\|$ is small.

- Similar to the Bai-Gail algorithm, consider the scaled dual attack lattice \[Λ(\vec{A}) = \{(\vec{x}, \vec{y}/c) \in \ZZ^m × {({1}/{c} ⋅ \ZZ)}^n : \vec{x} ⋅ \vec{A} ≡ \vec{y} \bmod q\} \] for some constant $c$.

- Lattice reduction produces a vector $(\vec{v}',\vec{w}')$ with \[\|(\vec{v}',\vec{w}')\| ≈ δ_0^{(m+n)}⋅ {(q/c)}^{n/(m+n)}.\]

- The final error we aim to distinguish from uniform is
\[e = \vec{v}' ⋅ \vec{A} ⋅ \vec{s} + \ip{\vec{v}'}{\vec{e}}  = \ip{c⋅ \vec{w}'}{\vec{s}} + \ip{\vec{v}'}{\vec{e}}.\]

** Modulus Switching for Dual Attack

From \[e = \vec{v}' ⋅ \vec{A} ⋅ \vec{s} + \ip{\vec{v}'}{\vec{e}}  = \ip{c⋅ \vec{w}'}{\vec{s}} + \ip{\vec{v}'}{\vec{e}}.\] we find $c$ by solving \[\sqrt{h}\,c = \frac{α\,q}{\sqrt{2\,\pi}} ⋅ \sqrt{m - n}\] which equalises the noise contributions of both parts of the sum.

** Modulus Switching for Dual Attack

***                                                                  :B_lemma:
:PROPERTIES:
:BEAMER_env: lemma
:END:

Let *$m=2\,n$* and \(c = \frac{α\,q}{\sqrt{2\,\pi\,h}} ⋅ \sqrt{m - n}\). A lattice reduction algorithm achieving $δ_0$ such that 

\[\log δ_0 = \frac{\log\left(\frac{\sqrt{8\,π}\, (\log(ε)/π)\, \sqrt{n}}{\left(2 \, π + 1\right)\, α
\sqrt{h}}\right)}{4 \, n}\]

leads to an algorithm solving decisional LWE with $\vec{s} \sample \B[64]{-}$ instance with advantage $ε$ and the same cost.

** Modulus Switching: Sieving

#+BEGIN_CENTER
#+BEGIN_EXPORT LaTeX
\begin{tikzpicture}[scale=0.95]
  \begin{axis}[
    legend pos=outer north east,
    axis lines=middle,
    xmin=0.5, xmax=3.5,
    ymin=120, ymax=150,
    axis y line*=left,
    axis x line*=bottom,
    xticklabels={Dual, Decode, Kannan},
    xtick={1,...,3},
    ytick={120, 127.3, 150},
    x tick label style={rotate=45,anchor=east},
    legend columns=1,
    legend cell align=left,]
    \addplot [only marks,color=skyblue1!40!white] table [x=idx,y=base/sieve,col sep=comma] {times.csv};
    \addplot [only marks,color=skyblue1,table/row sep=\\] table {
      1 127.3\\
      2 138.7\\
      3 140.1\\
    };
    \legend{base line, mod switch}\;
    \addplot[color=lightgray,table/row sep=\\] table {
      0.5 127.3\\
      3.5 127.3\\
    };
  \end{axis}
\end{tikzpicture}
#+END_EXPORT
#+END_CENTER

*** Code                                                            :noexport:

#+BEGIN_SRC sage
attach("small_sparse_secret.py")
n, alpha, q = fhe_params(n=2048, L=2)
sis_small_secret_mod_switch(n, alpha, q, secret_bounds=(-1,1), h=64, optimisation_target="sieve")["sieve"].log(2).n()
#+END_SRC

#+RESULTS:
: 127.335265009591

#+BEGIN_SRC sage
attach("small_sparse_secret.py")
n, alpha, q = fhe_params(n=2048, L=2)
sis_small_secret_mod_switch(n, alpha, q, secret_bounds=(-1,1), h=64, optimisation_target="lp")["lp"].log(2).n()
#+END_SRC

#+RESULTS:
: 159.588706627250

* Sparse Secrets
** Exploiting Sparse Secrets

Approaches so far exploit *small* secrets, but in HELib the secret is *sparse*, i.e. most components are zero.

** $\ZZ_q^n ≈ \ZZ_{q^2}^{n/2} ≈ \ZZ_{q^n}$ 

LWE in dimension $n$ and with modulus $q$ is equivalent to LWE in dimension $n/k$ and modulus $q^{k}$.footfullcite:STOC:BLPRS13

Let $n=2$, $A = \vec{a}_0 ⋅ q + \vec{a}_1 \pmod{q^2}$ and $S = \vec{s}_0  + \vec{s}_1 ⋅ q \pmod{q^2}$.

#+BEGIN_EXPORT LaTeX
\begin{align*}
A ⋅ S &= (\vec{a}_0 ⋅ q + \vec{a}_1) ⋅ (\vec{s}_0  + \vec{s}_1 ⋅ q) & \pmod{q^2}\\
     &= \vec{a}_0 ⋅ q ⋅ \vec{s}_0 + \vec{a}_1 ⋅ \vec{s}_0  + \vec{a}_0 ⋅ q ⋅ \vec{s}_1 ⋅ q + \vec{a}_1 ⋅ \vec{s}_1 ⋅ q  & \pmod{q^2}\\
     &= (\vec{a}_0 ⋅ \vec{s}_0 + \vec{a}_1 ⋅ \vec{s}_1) ⋅ q + \vec{a}_0 ⋅ \vec{s}_1 ⋅ q^2 + \vec{a}_1 ⋅ \vec{s}_0  & \pmod{q^2}\\
     &≈ (\ip{\vec{a}}{\vec{s}}\pmod{q}) ⋅ q & \pmod{q^2}
\end{align*}
#+END_EXPORT

** $\ZZ_q^n ≈ \ZZ_{q^2}^{n/2} ≈ \ZZ_{q^n}$ for Sparse Secrets

- Transform instance in dimension $n$ to instance in dimension $n/2$ and with modulus $q^2$.
- The new secret is $\vec{S}_i = \vec{s}_{2i+0}  + \vec{s}_{2i+1} ⋅ q \pmod{q^2}$ for $0≤ i < n/2$ where $\vec{s}_{2i+1} = 0$ with good probability.
- When this condition holds for all $\vec{S}_i$, the secret is shorter than the noise by a factor of $≈q$.
- Apply your favourite small secret solving strategy.

** Ignoring Components

- When the secret is sparse, most columns of $\vec{A}$ are irrelevant.

- In our example, the probability that a random coordinate is non-zero is \[64/2048 = 1/32.\]

- Ignoring $k$ random components in dimension $n$ for an instance with $h$ nonzero components will ignore only zero components with probability \[P_{k} = \prod_{i=0}^{k-1} \left(   1- \frac{h} {n-i} \right) = \frac{\binom{n-h}{k}}{\binom{n}{k}} \]

- Solving $≈1/P_{k}$ instances in dimension $n-k$ with sufficiently high advantage solves our instance at dimension $n$.

** Ignoring Components in Dual Attack

#+BEGIN_EXPORT LaTeX
\begin{center}
\scriptsize
\begin{align*}
0 \phantom{xxi} &\stackrel{?}{=} \overset{\vec{v}}{\begin{pmatrix}
v_0\\
v_1\\
v_2\\
\vdots\\
v_{m-3}\\
v_{m-2}\\
v_{m-1}\\
\end{pmatrix}} \cdot 
\overset{\vec{A}}{
\left(\begin{array}{ccc|ccc} 
a_{0,0} & \cdots & a_{0,k-1} & a_{0,k} & \cdots & a_{0,n-1}\\
a_{1,0} & \cdots & a_{1,k-1} & a_{1,k} & \cdots & a_{1,n-1}\\
a_{2,0} & \cdots & a_{2,k-1} & a_{2,k} & \cdots & a_{2,n-1}\\
\vdots  & \ddots & \vdots & \vdots & \ddots & \vdots\\
a_{m-3,0} & \cdots & a_{m-3,k-1} & a_{m-3,k} & \cdots & a_{m-3,n-1}\\
a_{m-2,0} & \cdots & a_{m-2,k-1} & a_{m-2,k} & \cdots & a_{m-2,n-1}\\
a_{m-1,0} & \cdots & a_{m-1,k-1} & a_{m-1,k} & \cdots & a_{m-1,n-1}\\
\end{array}\right)} \cdot 
\overset{\vec{s}}{\begin{pmatrix}s_0\\ \vdots\\ s_{k-1}\\ \hline s_k\\ \vdots\\ s_{n-1}\\ \end{pmatrix}}\\
&\stackrel{?}{=} \phantom{\begin{pmatrix}v_m-3\end{pmatrix} \cdot\ } \left(\begin{array}{ccc|ccc} 
\phantom{xxx} a'_{0,0} \phantom{x} & \cdots & \phantom{x} a'_{0,k-1} \phantom{xi} & \phantom{xxx} 0 \phantom{xxi} & \cdots & \phantom{xxxx} 0 \phantom{xxxx}\\
\end{array}\right) \phantom{i} \cdot {\begin{pmatrix}s_0\\ \vdots\\ s_{k-1}\\ \hline s_k\\ \vdots\\ s_{n-1}\\ \end{pmatrix}}\\
\end{align*}
\end{center}
#+END_EXPORT

** Ignoring Components in Dual Attack

#+BEGIN_EXPORT LaTeX
\begin{center}
\scriptsize
\begin{align*}
0 \phantom{xxi} &= \overset{\vec{v}}{\begin{pmatrix}
v_0\\
v_1\\
v_2\\
\vdots\\
v_{m-3}\\
v_{m-2}\\
v_{m-1}\\
\end{pmatrix}} \cdot 
\overset{\vec{A}}{
\left(\begin{array}{ccc|ccc} 
a_{0,0} & \cdots & a_{0,k-1} & a_{0,k} & \cdots & a_{0,n-1}\\
a_{1,0} & \cdots & a_{1,k-1} & a_{1,k} & \cdots & a_{1,n-1}\\
a_{2,0} & \cdots & a_{2,k-1} & a_{2,k} & \cdots & a_{2,n-1}\\
\vdots  & \ddots & \vdots & \vdots & \ddots & \vdots\\
a_{m-3,0} & \cdots & a_{m-3,k-1} & a_{m-3,k} & \cdots & a_{m-3,n-1}\\
a_{m-2,0} & \cdots & a_{m-2,k-1} & a_{m-2,k} & \cdots & a_{m-2,n-1}\\
a_{m-1,0} & \cdots & a_{m-1,k-1} & a_{m-1,k} & \cdots & a_{m-1,n-1}\\
\end{array}\right)} \cdot 
\overset{\vec{s}}{\begin{pmatrix}0\\ \vdots\\ 0\\ \hline s_k\\ \vdots\\ s_{n-1}\\ \end{pmatrix}}\\
&= \phantom{\begin{pmatrix}v_m-3\end{pmatrix} \cdot\ } \left(\begin{array}{ccc|ccc} 
\phantom{xxx} a'_{0,0} \phantom{x} & \cdots & \phantom{x} a'_{0,k-1} \phantom{xi} & \phantom{xxx} 0 \phantom{xxi} & \cdots & \phantom{xxxx} 0 \phantom{xxxx}\\
\end{array}\right) \phantom{i} \cdot {\begin{pmatrix}0\\ \vdots\\ 0\\ \hline s_k\\ \vdots\\ s_{n-1}\\ \end{pmatrix}}\\
\end{align*}
\end{center}
#+END_EXPORT

** Dual Attack

#+BEGIN_SRC sage :file sparse.png :exports results
def hypergeom(n, h, k, fail=0):
    N = n
    K = n-h
    n_ = k
    k_ = n_ - fail
    return (binomial(K,k_)*binomial(N-K,n_-k_)) / binomial(N,n_)

kwds = {"figsize":[12, 6], "thickness":2, "axes_labels": ['k','cost'], "dpi":300r, "transparent":True}

f = lambda k: (1/hypergeom(n,h,k)*sis(n-k, alpha, q, optimisation_target="sieve")["sieve"]).log(2)

l1 = line([(k,f(k)) for k in range(1,1200,32)], rgbcolor=skyblue1, legend_label="sieve", **kwds)

f = lambda k: (1/hypergeom(n,h,k)*sis(n-k,alpha,q,optimisation_target="lp")["lp"]).log(2)

l2 = line([(k,f(k)) for k in range(500,1600,32)], rgbcolor=chocolate1, legend_label="lp", **kwds)

l1+l2
#+END_SRC

#+ATTR_LATEX: :width 1.0\textwidth
#+RESULTS:
[[file:sparse.png]]


#+BEGIN_CENTER
Solving $1/P_k$ instances with $n=2048-k$, $α≈2^{-60.4}$ and $q≈2^{63.4}$.
#+END_CENTER

** Postprocessing

#+BEGIN_EXPORT LaTeX
\begin{center}
\scriptsize
\begin{align*}
\alert{a'_{0,0}} &= \overset{\vec{v}}{\begin{pmatrix}
v_0\\
v_1\\
v_2\\
\vdots\\
v_{m-3}\\
v_{m-2}\\
v_{m-1}\\
\end{pmatrix}} \cdot 
\overset{\vec{A}}{
\left(\begin{array}{ccc|ccc} 
a_{0,0} & \cdots & a_{0,k-1} & a_{0,k} & \cdots & a_{0,n-1}\\
a_{1,0} & \cdots & a_{1,k-1} & a_{1,k} & \cdots & a_{1,n-1}\\
a_{2,0} & \cdots & a_{2,k-1} & a_{2,k} & \cdots & a_{2,n-1}\\
\vdots  & \ddots & \vdots & \vdots & \ddots & \vdots\\
a_{m-3,0} & \cdots & a_{m-3,k-1} & a_{m-3,k} & \cdots & a_{m-3,n-1}\\
a_{m-2,0} & \cdots & a_{m-2,k-1} & a_{m-2,k} & \cdots & a_{m-2,n-1}\\
a_{m-1,0} & \cdots & a_{m-1,k-1} & a_{m-1,k} & \cdots & a_{m-1,n-1}\\
\end{array}\right)} \cdot 
\overset{\vec{s}}{\begin{pmatrix}\alert{1}\\ \vdots\\ 0\\ \hline s_k\\ \vdots\\ s_{n-1}\\ \end{pmatrix}}\\
&= \phantom{\begin{pmatrix}v_m-3\end{pmatrix} \cdot\ } \left(\begin{array}{ccc|ccc} 
\phantom{xxx} \alert{a'_{0,0}} \phantom{x} & \cdots & \phantom{x} a'_{0,k-1} \phantom{xi} & \phantom{xxx} 0 \phantom{xxi} & \cdots & \phantom{xxxx} 0 \phantom{xxxx}\\
\end{array}\right) \phantom{i} \cdot {\begin{pmatrix}\alert{1}\\ \vdots\\ 0\\ \hline s_k\\ \vdots\\ s_{n-1}\\ \end{pmatrix}}\\
\end{align*}
\end{center}
#+END_EXPORT

** Postprocessing

The probability to drop $k-j$ columns with $s_i=0$ and exactly $j$ components with $s_i ≠ 0$ is \[P_{k,j} =  \frac {{\binom{n-h}{k-j}}{\binom{h}{j}}}{\binom{n}{k}}\]

- Repeat experiment ${\left({\sum_{j=0}^{ℓ} P_{k,j}}\right)}^{-1}$ times

- Perform $\sum_{i=0}^{ℓ} \binom{k}{i} ⋅ 2^i$ checks against uniform distribution, reusing short vector output by lattice reduction.

** Ignoring Components

#+BEGIN_CENTER
#+BEGIN_EXPORT LaTeX
\begin{tikzpicture}[scale=0.95]
  \begin{axis}[
    legend pos=outer north east,
    axis lines=middle,
    xmin=0.5, xmax=3.5,
    ymin=50, ymax=190,
    axis y line*=left,
    axis x line*=bottom,
    xticklabels={Dual, Decode, Embed},
    xtick={1,...,3},
    ytick={104.4, 150},
    x tick label style={rotate=45,anchor=east},
    legend columns=1,
    legend cell align=left,]

    \addplot [only marks,color=chocolate1] table [x=idx,y=sparse/lp,col sep=comma] {times.csv};
    \addplot [only marks,color=skyblue1] table [x=idx,y=sparse/sieve,col sep=comma] {times.csv};

    \legend{lp,sieving}\;
 
    \addplot [only marks,color=skyblue1!40!white] table [x=idx,y=base/sieve,col sep=comma] {times.csv};
    \addplot [only marks,color=chocolate1!40!white] table [x=idx,y=base/lp,col sep=comma] {times.csv};
 
   \addplot[color=lightgray,table/row sep=\\] table {
      0.5 104.4\\
      3.5 104.4\\
    };

  \end{axis}
\end{tikzpicture}
#+END_EXPORT
#+END_CENTER

* Results

** Results

#+BEGIN_SRC csv :tangle times.csv :exports none
idx, base/sieve, base/lp, mod/sieve, mod/lp, sec/sieve, sec/lp, sparse/sieve, sparse/lp
1,   145.6,      188.9,   146.9,     191.3,  143.3,     185.2,  107.4,        104.1
2,   137.4,      174.7,   138.7,     177.4,  135.0,     171.6,  126.1,        113.5
3,   138.8,      178.6,   140.1,     180.9,  134.8,     171.9,  127.3,        111.5
#+END_SRC


|   |  Strategy |  Dual |       |   Dec |       | Embed |       |
|   |           | sieve |    lp | sieve |    lp | sieve |    lp |
|   |       <r> |       |       |       |       |       |       |
|---+-----------+-------+-------+-------+-------+-------+-------|
| / |         > |       |       |       |       |       |       |
| 0 | base line | 145.6 | 188.9 | 137.4 | 174.7 | 138.8 | 178.6 |
| 1 |  secret ↔ | 143.3 | 185.2 | 135.0 | 171.6 | 134.8 | 171.9 |
| 2 | modulus ↔ | 127.4 | 159.5 | 138.7 | 177.4 | 140.1 | 180.9 |
| 3 |      drop | 107.3 | 104.1 | 126.1 | 113.5 | 127.3 | 111.5 |
|---+-----------+-------+-------+-------+-------+-------+-------|
| 4 |        ++ |  96.8 |  92.9 | 125.4 | 113.2 | 127.3 | 111.5 |


After dropping some components the resulting instance still has a sparse and small secret → combine strategies: “++“.

*** Code                                                            :noexport:

#+BEGIN_SRC sage
print cost_str(drop_and_solve(sis_small_secret_mod_switch, n, alpha, q, secret_bounds=(-1,1), h=64, optimisation_target="sieve", postprocess=True))

print cost_str(drop_and_solve(sis_small_secret_mod_switch, n, alpha, q, secret_bounds=(-1,1), h=64, optimisation_target="lp", postprocess=True))
#+END_SRC

#+RESULTS:
: bop:   ≈2^97.3,  sieve:   ≈2^96.8,  oracle:   ≈2^22.5,  δ_0: 1.0075174,  bkz2:   ≈2^99.8,  k:       640,  lp:   ≈2^98.7,  repeat: 2040.6180,  dim:   ≈2^11.5,  postprocess:        10
: bop:   ≈2^93.4,  lp:   ≈2^92.9,  oracle:   ≈2^63.7,  δ_0: 1.0105230,  bkz2:  ≈2^104.4,  k:      1023,  sieve:  ≈2^123.3,  repeat:   ≈2^52.6,  dim:   ≈2^11.1,  postprocess:         4

** Thank you

#+BEGIN_CENTER
 [[./kitten-01.jpg]]

*@@beamer:\Large@@ Questions?*
#+END_CENTER

* Build Artefacts                                                     :noexport:
** Emacs Config

   #+BEGIN_SRC emacs-lisp :tangle .dir-locals.el
((magit-mode .
             ((eval .
                    (and
                     (visual-line-mode 1)))))
 (bibtex-mode . ((fill-column . 10000)))
 (org-mode .
           ((org-tags-column . -80)
            (eval .
                  (and
                   (flyspell-mode t)
                   (visual-fill-column-mode t))))))
   #+END_SRC

** Makefile

   #+BEGIN_SRC makefile :tangle Makefile
EMACS=emacs
EMACSFLAGS=--batch -l ~/.emacs.d/org-export-init.el
LATEXMK=latexmk
LATEXMKFLAGS=-xelatex

%.pdf: %.tex talk-header.tex
$(LATEXMK) $(LATEXMKFLAGS) $<

%.tex: %.org
$(EMACS) $(EMACSFLAGS) $< -f org-latex-export-to-latex

clean:
rm -f *.bbl *.aux *.out *.synctex.gz *.log *.run.xml *.blg *-blx.bib *.fdb_latexmk *.fls *.toc

.PHONY: clean all
.PRECIOUS: %.tex
   #+END_SRC

** Autoexport to PDF

   # Local Variables:
   # eval: (add-hook 'after-save-hook (lambda () (when (eq major-mode 'org-mode) (org-beamer-export-to-latex))) nil t)
   # End:

* Footnotes

[fn:1] See Léo’s talk for attacks on LTV and YASHE exploiting that they are not quite LWE.

[fn:2] See Damien’s talk on lattice reduction (and fplll’s implementation).


