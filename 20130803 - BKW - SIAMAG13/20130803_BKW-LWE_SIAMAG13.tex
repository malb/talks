\documentclass[10pt]{beamer}
\usepackage[utf8x]{inputenc}
\usepackage{xspace}
\usepackage{tikz}
\usepackage{relsize}
\usepackage{comment}

\mode<presentation>
{
  \setbeamercovered{transparent}
  \setbeamercolor{normal text}{fg=white,bg=gray}
  \setbeamercolor{alerted text}{fg=white}
  \setbeamercolor{example text}{fg=white}
  \setbeamercolor{background canvas}{bg=darkgray} 
  \setbeamercolor{structure}{fg=white}

  \setbeamercolor{block title}{bg=orange,fg=white}
  \setbeamercolor{block body}{bg=white,fg=darkgray}

  \setbeamercolor{palette primary}{use=structure,fg=structure.fg}

  \setbeamercolor{math text}{}
  \setbeamercolor{math text inlined}{parent=math text}
  \setbeamercolor{math text displayed}{parent=math text}

  \setbeamercolor{normal text in math text}{}

  \setbeamercolor{local structure}{parent=structure}

  \setbeamercolor{titlelike}{parent=structure}

  \setbeamercolor{title}{parent=titlelike}
  \setbeamercolor{title in head/foot}{parent=palette quaternary}
  \setbeamercolor{title in sidebar}{parent=palette sidebar quaternary}

  \setbeamercolor{subtitle}{parent=title}
}

\newcommand{\vectorbound}{\ensuremath{\kappa}}
\newcommand{\secretbound}{\ensuremath{\nu}}
\newcommand{\E}{\ensuremath{\textnormal{E}}}
\newcommand{\Var}{\ensuremath{\textnormal{Var}}}
\newcommand{\U}[1]{\ensuremath{\mathcal{U}(#1)\xspace}}
\newcommand{\abs}[1]{\ensuremath{|#1|}\xspace}
\newcommand{\dotp}[2]{\ensuremath{\left\langle {#1},{#2}\right\rangle}\xspace}

\newcommand{\shortvec}[1]{\tilde{\mathbf{#1}}\xspace}
\renewcommand{\vec}[1]{\mathbf{#1}\xspace}
\newcommand{\cemph}[1]{\color{pink}{#1}\xspace}
\newcommand{\chig}{\ensuremath{\chi_{\alpha,q}}}
\newcommand{\Z}{\ensuremath{\mathbb{Z}}\xspace}
\newcommand{\Zq}{\ensuremath{\Z_q}\xspace}
\newcommand{\Zp}{\ensuremath{\mathbb{Z}_p}\xspace}
\newcommand{\Ldis}{L_{\mathbf{s},\chi}^{(n)}\xspace}
\newcommand{\sample}{\ensuremath{\leftarrow_{\$}}}
\renewcommand{\O}[1]{\ensuremath{{\mathcal{O}\left(#1\right)}}\xspace}
\newcommand{\round}[1]{\ensuremath{\left\lfloor{#1}\right\rceil}\xspace}

\newtheorem{assumption}{Assumption}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Presentation Title Content %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Linear Algebra with Errors: On the Complexity of the Learning with Errors Problem}
\author[Martin R.\ Albrecht]{Martin R.\ Albrecht}
\institute[Universities of Somewhere and Elsewhere] % (optional, but mostly needed)
{joint work with C.\ Cid, J-C.\ Faugère, R.\ Fitzpatrick, and L.\ Perret}

\date{SIAM AG'13}

\AtBeginSection[] {
	\begin{frame}
		\frametitle{Contents}
		\tableofcontents[sectionstyle=show/shaded,subsectionstyle=show/show/hide]
	\end{frame}
}

\begin{document}

\begin{frame}[plain] % frame of type 'plain' is an empty frame
  \titlepage
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Table of Contents Slide %
%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

\begin{frame}{Learning with Errors}
\begin{definition}[Learning with Errors]
\begin{itemize}
\item Let $n\geq 1$, $m \gg n$, $q$ odd, $\chi$ be a probability distribution on $\Z_q$ and $\vec{s}$ be a secret vector in $\Z_q^n$.
\pause
\item Let $\vec{e} \sample \chi^m$, $\vec{A} \sample \U{\Z_q^{m \times n}}$. We denote by $\Ldis$ the distribution on $\Z_q^{m \times n} \times \Z_q^m$ produced as $(\vec{A}, \vec{A} \cdot \vec{s} + \vec{e})$.
\pause
\item Decision-LWE is the problem of deciding whether $\vec{A},\vec{c} \sample \U{\Z_q^{m\times n} \times \Z_q^{m}}$ or $\vec{A},\vec{c} \sample \Ldis$.
\end{itemize}
\end{definition}

\pause

In other words: Is $\vec{c}$ sampled uniformly randomly or is it $\vec{A} \cdot \vec{s} + \vec{e}$ where typically $\vec{e}$ is ``small''.

\vspace{1em}

Typically, $\chi$ is a discrete Gaussian distribution with small standard deviation.

\end{frame}

\begin{frame}{Applications}

\begin{itemize}
\item Public-Key Encryption, Digital Signature Schemes
\item Identity-based Encryption: encrypting to an identity (e-mail address \dots) instead of key
\item Fully-homomorphic encryption: computing with encrypted data
\item \dots
\end{itemize}
\end{frame}

\begin{frame}{Asymptotic Security}
Reduction of average-case LWE to (assumed) worst-case hard lattice problems such as Closest Vector Problem (CVP).

\vspace{1em}\pause
 
But to build cryptosystems we need to understand the hardness of concrete instances: Given $m, n, q$ and $\chi$ how many operations does it take to solve Decision-LWE?

\end{frame}


\begin{frame}{Solving Strategies}

Given $\vec{A},\vec{c}$ with $\vec{c} = \vec{A} \cdot \vec{s} + \vec{e}$ solve the problem in the primal lattice or the dual lattice.

\vspace{1em}

\begin{itemize}

\item Solve the Bounded-Distance Decoding ($\mathrm{BDD}$) problem in the primal lattice: Find $\vec{s'}$ such that
\[
 \|\vec{y} - \vec{c}\| \textnormal{ is minimised, for } \vec{y} = \vec{A} \cdot \vec{s'}.
\]

\pause

\item Solve the Short-Integer-Solutions ($\mathrm{SIS}$) problem in the scaled dual lattice. Find a short $\vec{y}$ such that
\[
 \vec{y} \cdot \vec{A} = 0 \textnormal{ and check if } \dotp{\vec{y}}{\vec{c}} = \vec{y}\cdot \left(\vec{A} \cdot \vec{s} + \vec{e}\right) = \dotp{\vec{y}}{\vec{e}} \textnormal{ is short.}
\]
\end{itemize}

\pause

\begin{block}{In this talk}
\begin{itemize}
 \item solving SIS using combinatorial techniques and
 \item no bound on $m$.
\end{itemize}
\end{block}


\end{frame}


\section{Warm-Up: Deciding Consistency in Noise Free Systems}

\begin{frame}
\frametitle{Gaussian elimination}
Asume $\vec{e} = \vec{0}$, we hence want to decide whether there is a solution $\vec{s}$ such that $\vec{c} = \vec{A}\cdot \vec{s}$. We may apply Gaussian elimination to the matrix:
\[
\vec{[A\mid c]} = \left(\begin{array}{cccc|l}
\bold{a}_{11} & \bold{a}_{12} & \dots & \bold{a}_{1n} & c_1\\
\bold{a}_{21} & \bold{a}_{22} & \dots & \bold{a}_{2n} & c_2\\
\vdots & \vdots & \ddots & \vdots & \vdots\\
\bold{a}_{m1} & \bold{a}_{m2} & \dots & \bold{a}_{mn} & c_m\\
\end{array}\right)
\]
to recover
\[
\vec{[\tilde A\mid \tilde c]} = \left(\begin{array}{cccc|l}
\bold{a}_{11} &        \bold{a}_{12} &  \dots & \bold{a}_{1n}        & c_1\\
            0 & \tilde{\bold{a}}_{22} &  \dots & \tilde{\bold{a}}_{2n} & \tilde c_2\\
       \vdots & \vdots               & \ddots & \vdots               & \vdots\\
            0 &      0               &  \dots & \tilde{\bold{a}}_{mn} & \tilde c_n\\
       \vdots & \vdots               & \ddots & \vdots               & \vdots\\
    0 & 0 & \dots & 0 &\tilde  c_m\\
\end{array}\right)
\]

If and only if $\tilde c_{n+1},\dots,\tilde c_{m}$ are all zero, the system is consistent.
\end{frame}


\section{Solving Decision-LWE}

\begin{frame}[allowframebreaks]
\frametitle{BKW Algorithm}

The BKW algorithm was first proposed for the Learning Parity with Noise (LPN) problem which can be viewed as a special case of LWE.

\vspace{1em}

\begin{thebibliography}{foobar}
\bibitem{DBLP:journals/jacm/BlumKW03}
Avrim Blum, Adam Kalai, and Hal Wasserman.
\newblock Noise-tolerant learning, the parity problem, and the statistical query model.
\newblock {\em J. ACM}, 50(4):506--519, 2003.
\end{thebibliography}

\framebreak

We revisit Gaussian elimination:
\begin{eqnarray*}
& & \left(
\begin{array}{c|c|ccc|l}
\bold{a}_{11} & \bold{a}_{12} & \bold{a}_{13} & \cdots & \bold{a}_{1n} & c_1\\
\bold{a}_{21} & \bold{a}_{22} & \bold{a}_{23} & \cdots & \bold{a}_{2n} & c_2\\
\vdots & \vdots & \ddots & \vdots & \vdots\\
\bold{a}_{m1} & \bold{a}_{m2} & \bold{a}_{m3} & \cdots & \bold{a}_{mn} & c_{m}
\end{array}
\right)\\
& = & \left(
\begin{array}{c|c|ccc|l}
\bold{a}_{11} & \bold{a}_{12} & \bold{a}_{13} & \cdots & \bold{a}_{1n} & \dotp{\vec{a}_1}{\vec{s}} + \vec{e}_1\\
\bold{a}_{21} & \bold{a}_{22} & \bold{a}_{23} & \cdots & \bold{a}_{2n} & \dotp{\vec{a}_2}{\vec{s}} + \vec{e}_2\\
\vdots & \vdots & \ddots & \vdots & \vdots\\
\bold{a}_{m1} & \bold{a}_{m2} & \bold{a}_{m3} & \cdots & \bold{a}_{mn} & \dotp{\vec{a}_m}{\vec{s}} + \vec{e}_m
\end{array}
\right)\\
& \Rightarrow  & \left(
\begin{array}{c|c|ccc|l}
\bold{a}_{11} & \bold{a}_{12} & \bold{a}_{13} & \cdots & \bold{a}_{1n} & \dotp{\vec{a}_1}{\vec{s}} + \vec{e}_1\\
0 & \tilde{\bold{a}}_{22} & \tilde{\bold{a}}_{23} & \cdots & \tilde{\bold{a}}_{2n} & \dotp{\shortvec{a}_2}{\vec{s}} + \vec{e}_2 - \frac{\bold{a}_{21}}{\bold{a}_{11}}\vec{e}_1\\
\vdots & \vdots & \ddots & \vdots & \vdots\\
0 & \tilde{\bold{a}}_{m2} & \tilde{\bold{a}}_{m3} & \cdots & \tilde{\bold{a}}_{mn} & \dotp{\shortvec{a}_m}{\vec{s}} + \vec{e}_m - \frac{\bold{a}_{m1}}{\bold{a}_{11}}\vec{e}_1
\end{array}
\right)\phantom{\Longrightarrow}
\end{eqnarray*}

\framebreak

\begin{itemize}
 \item $\frac{\bold{a}_{i1}}{\bold{a}_{11}}$ is essentially a random element in $\Z_q$, hence $\tilde c_i \sample \U{\Z_q}$.
 \item Even if $\frac{\bold{a}_{i1}}{\bold{a}_{11}}$ is 1 the variance of the noise doubles at every level because of the addition.  
\end{itemize}

\vspace{1em}

\begin{block}{Setting}
\begin{itemize}
 \item \textbf{Problem:} additions and multiplications $\Rightarrow$ noise of $\tilde{c}$ values increases rapidly
 \item \textbf{Strategy:} exploit that we have many rows: $m \gg n$.
\end{itemize}
\end{block} 

\framebreak

We considering $a \approx \log n$ `blocks' of $b$ elements each.

\begin{equation*}
\left(
\begin{array}{cc|ccc|c}
\bold{a}_{11} & \bold{a}_{12} & \bold{a}_{13} & \cdots & \bold{a}_{1n} & c_0\\
\bold{a}_{21} & \bold{a}_{22} & \bold{a}_{23} & \cdots & \bold{a}_{2n} & c_1\\
\vdots & \vdots & \ddots & \vdots & \vdots\\
\bold{a}_{m1} & \bold{a}_{m2} & \bold{a}_{m3} & \cdots & \bold{a}_{mn} & c_{m}
\end{array}
\right)
\end{equation*}

\framebreak

For each block we build a table of all $q^b$ possible values.

\begin{equation*}
T = \left[ 
\begin{array}{cc|ccc|c}
0 & 0 & \bold{a}_{13} & \cdots & \bold{a}_{1n} & c_0\\
0 & 1 & \bold{a}_{23} & \cdots & \bold{a}_{2n} & c_1\\
\vdots & \vdots & \ddots & \vdots & \vdots\\
q & q & \bold{a}_{q^23} & \cdots & \bold{a}_{q^2n} & c_{q^2}
\end{array}\right]
\end{equation*}

\framebreak

We use these tables to eliminate $b$ entries in other rows.

\begin{eqnarray*}
& & \left(
\begin{array}{cc|ccc|c}
\bold{a}_{11} & \bold{a}_{12} & \bold{a}_{13} & \cdots & \bold{a}_{1n} & c_0\\
\bold{a}_{21} & \bold{a}_{22} & \bold{a}_{23} & \cdots & \bold{a}_{2n} & c_1\\
\vdots & \vdots & \ddots & \vdots & \vdots\\
\bold{a}_{m1} & \bold{a}_{m2} & \bold{a}_{m3} & \cdots & \bold{a}_{mn} & c_{m}
\end{array}
\right)\\
&+& \left[
\begin{array}{cc|ccc|c}
0 \hspace{1.1em} & 0 \hspace{1.1em} & \bold{a}_{13} & \cdots & \bold{a}_{1n} & c_0\\
0 \hspace{1.1em} & 1 \hspace{1.1em} & \bold{a}_{23} & \cdots & \bold{a}_{2n} & c_1\\
\vdots \hspace{1.1em} & \vdots \hspace{1.1em} & \ddots & \vdots & \vdots\\
q \hspace{1.1em} & q \hspace{1.1em} & \bold{a}_{q^23} & \cdots & \bold{a}_{q^2n} & c_{q^2}
\end{array}\right]\\
&\Rightarrow& \left(
\begin{array}{cc|ccc|c}
\bold{a}_{11} & \bold{a}_{12} & \bold{a}_{13} & \cdots & \bold{a}_{1n} & c_0\\
0 & 0 & \bold{a}_{23} & \cdots & \tilde{\bold{a}_{2n}} & \tilde{c}_1\\
\vdots & \vdots & \ddots & \vdots & \vdots\\
0 & 0 & \tilde{\bold{a}}_{m3} & \cdots & \tilde{\bold{a}}_{mn} & \tilde{c}_{m}
\end{array}
\right)\phantom{\Longrightarrow}
\end{eqnarray*}

\framebreak

This gives a time complexity of $$\approx (a^2n)\cdot\frac{q^b}{2}$$ and a memory requirement of $$\approx\frac{q^b}{2}\cdot a\cdot(n+1).$$

\vspace{1em}

A detailed analysis of the algorithm for LWE is available as:

\begin{thebibliography}{foobar}
\bibitem{foobar}
Martin R.\ Albrecht, Carlos Cid, Jean-Charles Faugère, Robert Fitzpatrick and Ludovic Perret
\newblock On the Complexity of the BKW Algorithm on LWE
\newblock {\em ePrint Report} 2012/636, 2012.
\newblock to appear in {\em Designs, Codes and Cryptography}.
\end{thebibliography}
\end{frame}

\section{Solving Decision-LWE with Small Secrets}

\begin{frame}
\frametitle{The Setting}

Assume $\vec{s} \sample \U{\Z_2^n}$, i.e. all entries in $\vec{s}$ are very small.

\vspace{1em}

This is a common setting in cryptography for performance reasons and because this allows to realise some advanced schemes. In particular, a technique called `modulus switching' can be used to improve the performance of homomorphic encryption schemes.

\vspace{1em}

\begin{thebibliography}{foobar}
\bibitem{brakerski-vaikuntanathan:focs2011}
Zvika Brakerski and Vinod Vaikuntanathan.
\newblock Efficient fully homomorphic encryption from (standard) {LWE}.
\newblock In Rafail Ostrovsky, editor, {\em IEEE 52nd Annual Symposium on
  Foundations of Computer Science, FOCS 2011}, pages 97--106. IEEE, 2011.
\end{thebibliography}

\end{frame}

\begin{frame}[allowframebreaks]
\frametitle{Modulus Reduction}

Given a sample $(\vec{a},c)$ where $c = \dotp{\vec{a}}{\vec{s}} + e$ and some $p < q $ we may consider $$\left(\round{\frac{p}{q} \cdot \vec{a}}, \round{\frac{p}{q} \cdot c}\right)$$ with
\begin{eqnarray*}
\round{\frac{p}{q} \cdot c} &=& \round{ \dotp{ \frac{p}{q} \cdot \vec{a} }{\vec{s} } + \frac{p}{q} \cdot e}\\
  &=& \round{ \dotp{ \round{ \frac{p}{q} \cdot \vec{a} }}{\vec{s} } + \dotp{\frac{p}{q} \cdot \vec{a} - \round{ \frac{p}{q} \cdot \vec{a} }}{\vec{s}} + \frac{p}{q} \cdot e}\\
  &=& \dotp{ \round{ \frac{p}{q} \cdot \vec{a} }}{\vec{s} } + \dotp{\frac{p}{q} \cdot \vec{a} - \round{ \frac{p}{q} \cdot \vec{a} }}{\vec{s}} + \frac{p}{q} \cdot e \pm [0,0.5]\\
  &=& \dotp{ \round{ \frac{p}{q} \cdot \vec{a} }}{\vec{s} } + e''.\\
\end{eqnarray*}

\framebreak

\begin{block}{Example}
\begin{eqnarray*}
p, q &=& 10, 20\\
\vec{a} &=& (8, -2, 0, 4, 2, -7),\\
\vec{s} &=& (0, 1, 0, 0, 1, 1),\\
 \dotp{\vec{a}}{\vec{s}} &=& -7,\\
c &=& -6\\
\vec{a'} = \round{\frac{p}{q} \cdot \vec{a}} &=& (4, -1, 0, 2, 1, -4)\\
\dotp{\vec{a'}}{\vec{s}} &=& -4,\\
\round{\frac{p}{q} \cdot c} &=& -4.\\
\end{eqnarray*}
\end{block}


\framebreak

Typically, we would choose

$$p \approx q \cdot \sqrt{n \cdot \Var(\U{[-0.5,0.5]}) \cdot \sigma^2_s}/\sigma = q \cdot \sqrt{n/12} \sigma_s/\sigma$$ where $\sigma_s$ is the standard deviation of elements in $\vec{s}$.

\vspace{1em}

If $\vec{s}$ is small then $e''$ is small and we may compute with the smaller `precision' $p$ at the cost of a slight increase of the noise rate. 

\vspace{1em}

The complexity hence drops to $$\approx (a^2n)\cdot\frac{p^b}{2}$$ with $a$ usually is unchanged.
\end{frame}

\begin{frame}[allowframebreaks]
\frametitle{Lazy Modulus Switching} 
For simplicity assume $p = 2^\kappa$ and consider the LWE matrix
\begin{eqnarray*}
\footnotesize
[\vec{A} \mid \vec{c}] &=& \left(\begin{array}{ccccc}
       \vec{a}_{1,1} & \vec{a}_{1,2} & \hspace{2em} \dots \hspace{2em} & \vec{a}_{1,n} & c_1\\
       \vec{a}_{2,1} & \vec{a}_{2,2} & \dots & \vec{a}_{2,n} & c_2\\
        \vdots & \vdots & \ddots & \vdots & \vdots\\
       \vec{a}_{m,1} & \vec{a}_{m,2} & \dots & \vec{a}_{m,n} & c_{m}\\
      \end{array}\right)\\
\end{eqnarray*}
as
\begin{eqnarray*}
\footnotesize
[\vec{A} \mid \vec{c}] & = & \left(\begin{array}{cccccccc}
       \vec{a}_{1,1}^h & \vec{a}_{1,1}^{l} \hspace{1em} & \vec{a}_{1,2}^h & \vec{a}_{1,2}^{l} & \hspace{1em} \dots \hspace{1em} & \vec{a}_{1,n}^h & \vec{a}_{1,n}^{l}  \hspace{1em} & c_1\\
       \vec{a}_{2,1}^h & \vec{a}_{2,1}^{l} \hspace{1em} & \vec{a}_{2,2}^h & \vec{a}_{2,2}^{l} & \hspace{1em} \dots \hspace{1em} & \vec{a}_{2,n}^h & \vec{a}_{2,n}^{l}  \hspace{1em} & c_2\\
        \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots\\
       \vec{a}_{m,1}^h & \vec{a}_{m,1}^{l} \hspace{1em} & \vec{a}_{m,2}^h & \vec{a}_{m,2}^{l} & \hspace{1em} \dots \hspace{1em} & \vec{a}_{m,n}^h & \vec{a}_{m,n}^{l}  \hspace{1em} & c_{m}\\
      \end{array}\right)\\
\end{eqnarray*}
where $\vec{a}_{i,j}^h$ and $\vec{a}_{i,j}^{l}$ denote high and low order bits:
\begin{itemize}
 \item $\vec{a}_{i,j}^h$ corresponds to $\round{p/q \cdot \vec{a}_{i,j}}$ and 
 \item $\vec{a}_{i,j}^l$ corresponds to $\round{p/q \cdot \vec{a}_{i,j}} - p/q \cdot \vec{a}_{i,j}$, the rounding error.
\end{itemize}

In order to clear the most significant bits in every component of the $\vec{a}_{i}$, we run the BKW algorithm on the matrix $[\vec{A} \mid \vec{c}]$ but only consider
\begin{eqnarray*}
[\vec{A},\vec{c}]^h &:=& \left(\begin{array}{ccccc}
       \vec{a}_{1,1}^h & \vec{a}_{1,2}^h & \hspace{2em} \dots \hspace{2em} & \vec{a}_{1,n}^h & c_1\\
       \vec{a}_{2,1}^h & \vec{a}_{2,2}^h & \dots & \vec{a}_{2,n}^h & c_2\\
        \vdots & \vdots & \ddots & \vdots & \vdots\\
       \vec{a}_{m,1}^h & \vec{a}_{m,2}^h & \dots & \vec{a}_{m,n}^h & c_{m}\\
      \end{array}\right).
\end{eqnarray*}
when searching for collisions. 

\vspace{1em}

We only manage elimination tables for the most significant $\kappa$ bits.

All arithmetic is performed in $\Zq$ but collisions are searched for in $\Zp$.

\framebreak

\begin{itemize}
 \item We do not apply modulus reduction in one shot, but only when needed
 \item As a consequence rounding errors accumulate not as fast: they only start to accumulate when we branch on a component.
\end{itemize}

\vspace{1em}

\begin{block}{}
\centering We may reduce $p$ by a factor of $\sqrt{a/2}$.
\end{block}

This may translate to huge gains the complexity of BKW is $\approx p^b$ where typically $b \approx n/\log n$.

\end{frame}

\begin{frame}[allowframebreaks,fragile]
\frametitle{Stunting Growth}

\begin{figure}
\begin{tikzpicture}[
  font=,
  to/.style={-,shorten >=1pt,semithick,font=\footnotesize},scale=0.9
    ]

\node (box1) at (7, 0) {$\Box$};
\node (box2) at (7.3, 0) {$\Box$};
\node (line1) at (7.5, 0) {$\mid$};
\node (box3) at (7.7, 0) {$\Box$};
\node (box4) at (8.0, 0) {$\Box$};
\node (line2) at (8.2, 0) {$\mid$};
\node (box5) at (8.4, 0) {$\blacksquare$};
\node (box6) at (8.7, 0) {$\blacksquare$};
\node (line3) at (8.9, 0) {$\mid$};
\node (box7) at (9.1, 0) {$\blacksquare$};
\node (box8) at (9.4, 0) {$\blacksquare$};
\node (line4) at (9.6, 0) {$\mid$};
\node (box9) at (9.8, 0) {$\blacksquare$};
\node (box10) at (10.1, 0) {$\blacksquare$};

\node (rb1) at (7.1, 0.1) {};
\node (rb2) at (7.4, 0.1) {};

\node (rb3) at (7.8, 0.1) {};
\node (rb4) at (8.1, 0.1) {};

\node (rb5) at (8.3, 0.05) {};
\node (rb6) at (8.6, 0.05) {};

\node (ind1) at (7.05, -0.3) {$\vec{a}_{(0)}$};
\node (ind2) at (10.2, -0.3) {$\vec{a}_{(9)}$};

\node (label1) at (3.8, 1.2) {Children, from $T^0$};
\node (label2) at (5.5, 2.1) {Children, from $T^1$};
\node (label3) at (9, 1.6) {Parent entries (w.r.t. $T^2$)};
\node (label4) at (13, 2) {Strangers};

\draw[to] (label1) -- (rb1);
\draw[to] (label1) -- (rb2);

\draw[to] (label2) -- (rb3);
\draw[to] (label2) -- (rb4);

\draw[to] (label3) -- (rb5);
\draw[to] (label3) -- (rb6);

\draw[to] (label4) -- (box6);
\draw[to] (label4) -- (box7);
\draw[to] (label4) -- (box8);
\draw[to] (label4) -- (box9);

\node (dummy) at (15, 0) {};

\end{tikzpicture}
\hspace{\fill}
\caption{Children, parents and strangers.}
\label{fig:intuition}
\end{figure}

\framebreak

Assume $b=1$ and $a\geq 3$, for the outputs $(\shortvec{a}_i, \tilde c_i)$ where the first three components are reduced  have:
 
\begin{eqnarray*}
  \shortvec{a}_i &=& \vec{a}_i \mbox{ from } \Ldis\\
                 &+& \shortvec{a}_0 \mbox{ with } \shortvec{a}_0 \mbox{ from } T^0\\
                 &+& \shortvec{a}_1 \mbox{ with } \shortvec{a}_1 \mbox{ from } T^1\\
                 &+& \shortvec{a}_2 \mbox{ with } \shortvec{a}_2 \mbox{ from } T^2\\
\end{eqnarray*}

Considering component $\shortvec{a}_{i,(0)}$ we have that
\begin{itemize}
 \item $\vec{a}_{i,(0)}$ is uniform in $\Zq$,
 \item $\shortvec{a}_{0,(0)}$ reduces this to something of size $r  = \log_2 q - \log_2 p$
 \item $\shortvec{a}_{1,(0)}$ has size $\log_2 q - \log_2 p$
 \item $\shortvec{a}_{2,(0)}$ has size $\approx \log_2 q - \log_2 p + 1$, and \textbf{depends} on entries on $T^1$.
\end{itemize}

\framebreak

\begin{block}{}
We sample many candidates for $\shortvec{a}_{2}$ to find one where $\shortvec{a}_{2,(0)}$ is particularly small.
\end{block}

\vspace{1em}

This is easier than for $\shortvec{a}_{3}$ but influences $\shortvec{a}_3$.

\framebreak

\begin{assumption}
\label{ass:minvar}
Let the vectors $\vec{x}_i \in \Z_q^{\tau}$ be sampled from some distribution $\mathcal{D}$ such that $\sigma^2 = \Var(\vec{x}_{i,(j)})$ where $\mathcal{D}$ is any distribution on (sub-)vectors observable in our algorithm. Let $\vec{y} = \min_{abs}\left(\vec{x}_0,\dots,\vec{x}_{n-1}\right)$ where $\min_{abs}$ picks that vector $\vec{x}_{min}$ with $\sum_{j=0}^{b\cdot\ell-1} \abs{\vec{x}_{min,(j)}}$ minimal. The standard deviation $\sigma_{n} = \sqrt{\Var(\vec{y}_{(j)})}$ of components in $\vec{y}$ satisfies
$$\sigma/\sigma_n \geq c_\tau\, \sqrt[\tau]{n} + (1 - c_\tau)$$ with
$$c_\tau = 0.20151418166952917\,\sqrt{\tau}  + 0.32362108131969386\approx \frac{1}{5}\sqrt{\tau} + \frac{1}{3}.$$
\end{assumption}


\begin{thebibliography}{foobar}
\bibitem{foobar}
Martin R.\ Albrecht, Jean-Charles Faugère, Robert Fitzpatrick and Ludovic Perret
\newblock Lazy Modulus Switching for the BKW Algorithm on LWE
\newblock in preparation, 2013.

\end{thebibliography}

\end{frame}




\begin{frame}
\frametitle{Results}

\begin{table}[htbp]
\footnotesize
\centering
\begin{tabular}{|r||r|r||r|r|}
\hline
    & \multicolumn{2}{|c||}{BKW} & \multicolumn{2}{|c|}{+ Mod.\ Switch}\\
\hline
$n$ & $\log \Z_2$ & $\log \textnormal{mem}$ &$\log \Z_2$ & $\log \textnormal{mem}$\\
\hline
  32 &  40.0 &     26.2  &   39.4 &     25.5\\
  64 &  55.9 &     48.8  &   52.5 &     46.0\\
 128 &  97.6 &     90.0  &   89.6 &     81.2\\
 256 & 182.1 &    174.2  &  164.0 &    156.7\\
 512 & 361.0 &    352.8  &  305.6 &    297.9\\
1024 & 705.5 &    697.0  &  580.2 &    572.2\\
\hline
    & \multicolumn{2}{|c||}{This Work (1)} & \multicolumn{2}{|c|}{This Work (2)}\\
\hline
$n$ & $\log \Z_2$ & $\log \textnormal{mem}$ &$\log \Z_2$ & $\log \textnormal{mem}$\\
\hline
  32 &  40.0 &     26.1 &  40.0 &     26.1\\
  64 &  49.2 &     42.1 &  47.6 &     32.0\\
 128 &  78.2 &     70.8 &  74.2 &     46.3\\
 256 & 142.7 &    134.9 & 132.5 &     67.1\\
 512 & 251.2 &    243.1 & 241.8 &    180.0\\
1024 & 494.8 &    486.5 & 485.0 &    407.5\\
\hline
\end{tabular}
\caption{Cost for solving Decision-LWE with advantage $\approx 1$ for BKW and BKZ variants where $q$ and $\sigma$ are chosen as in Regev's scheme and $\vec{s} \sample \U{\Z_2^n}$ ``$\log \Z_2$'' gives the number of ``bit operations'' and ``$\log \textnormal{mem}$'' the memory requirement of $\Zq$ elements. All logarithms are base 2.}
\label{tab:modred}
\end{table}
\end{frame}

\begin{frame}{Fin}
\begin{center}
\large{Questions?}
\end{center}
\end{frame}

\end{document}


